{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ“ **Introduction**\n\nThis Python script demonstrates how to use the Pegasus model from Hugging Face's Transformers library to perform abstractive summarization on a given text. It evaluates the quality of the generated summaries using both lexical (ROUGE) and semantic (BERTScore) metrics. The model used, `google/pegasus-cnn_dailymail`, is fine-tuned specifically for summarizing news articles.\n\n---\n\n## ğŸ”§ Installation and Required Libraries\n\n```python\n!pip install transformers datasets rouge_score bert_score --quiet\n```\n\n* `transformers`: Provides state-of-the-art pre-trained models like Pegasus.\n* `datasets`: Useful for working with text datasets.\n* `rouge_score`: Used to calculate ROUGE scores for summary evaluation.\n* `bert_score`: Provides a semantic similarity score using BERT embeddings.","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets rouge_score bert_score --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:23:33.618110Z","iopub.execute_input":"2025-06-27T13:23:33.618407Z","iopub.status.idle":"2025-06-27T13:25:20.894786Z","shell.execute_reply.started":"2025-06-27T13:23:33.618382Z","shell.execute_reply":"2025-06-27T13:25:20.893375Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"---\n\n## ğŸ§© Importing Libraries\n\n```python\nimport logging\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom rouge_score import rouge_scorer\nfrom bert_score import score\n```\n\n* Necessary modules are imported.\n* `PegasusForConditionalGeneration`: Loads the Pegasus model for summarization.\n* `PegasusTokenizer`: Tokenizes text inputs for the model.\n","metadata":{}},{"cell_type":"code","source":"import logging\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom rouge_score import rouge_scorer\nfrom bert_score import score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:25:20.897543Z","iopub.execute_input":"2025-06-27T13:25:20.897980Z","iopub.status.idle":"2025-06-27T13:25:55.460035Z","shell.execute_reply.started":"2025-06-27T13:25:20.897928Z","shell.execute_reply":"2025-06-27T13:25:55.459114Z"}},"outputs":[{"name":"stderr","text":"2025-06-27 13:25:39.082795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751030739.349015      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751030739.430106      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"---\n\n## ğŸ”‡ Logging Configuration\n\n```python\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogging.getLogger(\"absl\").setLevel(logging.ERROR)\n```\n\n* Reduces verbosity by setting logging level to only show errors.\n","metadata":{}},{"cell_type":"code","source":"logging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogging.getLogger(\"absl\").setLevel(logging.ERROR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:25:55.460872Z","iopub.execute_input":"2025-06-27T13:25:55.461438Z","iopub.status.idle":"2025-06-27T13:25:55.466314Z","shell.execute_reply.started":"2025-06-27T13:25:55.461413Z","shell.execute_reply":"2025-06-27T13:25:55.465397Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"---\n\n## ğŸ¤– Loading the Model and Tokenizer\n\n```python\nmodel_name = \"google/pegasus-cnn_dailymail\"\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\n```\n\n* The Pegasus model fine-tuned on the CNN/DailyMail dataset is loaded.\n* Tokenizer converts input text into tokens.\n* The model uses these tokens to generate a summary.","metadata":{}},{"cell_type":"code","source":"model_name = \"google/pegasus-cnn_dailymail\"\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:25:55.468669Z","iopub.execute_input":"2025-06-27T13:25:55.468976Z","iopub.status.idle":"2025-06-27T13:26:15.592650Z","shell.execute_reply.started":"2025-06-27T13:25:55.468952Z","shell.execute_reply":"2025-06-27T13:26:15.583673Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd288cf2e6fa4e68a34fe1447ba2b8f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77be834b966c4bf79e170f68fabc58f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a76d3e0fcf64a33afb82dd159a0de50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7e0de0ce974151a6a0e932a2395379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d64405fd54d4c1d8917f6785460106a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8af91807a18d42f1a8baaa8f4c5be1d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab0e0e41b88f47e48cbc97f5e605288b"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"---\n\n## âœ‚ï¸ Summarization Function\n\n```python\ndef summarize(text, max_length=60):\n```\n\n* `text`: Input text to summarize.\n* `max_length`: Maximum length of the generated summary.\n\n```python\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        padding='max_length',\n        max_length=1024,\n        return_tensors=\"pt\"\n    )\n```\n\n* The text is tokenized.\n* Long texts are truncated.\n* Padding is applied to reach maximum length.\n* Input is converted to PyTorch tensors.\n\n```python\n    summary_ids = model.generate(\n        inputs['input_ids'],\n        max_length=max_length,\n        num_beams=4,\n        early_stopping=True\n    )\n```\n\n* The model generates a summary using beam search.\n* `num_beams=4`: Beam width for better output.\n* `early_stopping=True`: Stops early if an optimal summary is found.\n\n```python\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    summary = summary.replace(\"<n>\", \" \").strip()\n    return summary\n```\n\n* The generated summary is decoded back into text.\n* Special tokens are removed.\n\n","metadata":{}},{"cell_type":"code","source":"def summarize(text, max_length=60):\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        padding='max_length',\n        max_length=1024,\n        return_tensors=\"pt\"\n    )\n    summary_ids = model.generate(\n        inputs['input_ids'],\n        max_length=max_length,\n        num_beams=4,\n        early_stopping=True\n    )\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    summary = summary.replace(\"<n>\", \" \").strip()\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:26:15.597102Z","iopub.execute_input":"2025-06-27T13:26:15.599868Z","iopub.status.idle":"2025-06-27T13:26:15.616053Z","shell.execute_reply.started":"2025-06-27T13:26:15.599791Z","shell.execute_reply":"2025-06-27T13:26:15.612856Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"---\n\n## ğŸ“ ROUGE Evaluation Function\n\n```python\ndef evaluate_rouge(reference, summary):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    return scorer.score(reference, summary)\n```\n\n* `reference`: The original text.\n* `summary`: The generated summary.\n* `rouge1`: Unigram (word-level) overlap.\n* `rouge2`: Bigram overlap.\n* `rougeL`: Longest common subsequence.","metadata":{}},{"cell_type":"code","source":"def evaluate_rouge(reference, summary):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    return scorer.score(reference, summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:26:15.620855Z","iopub.execute_input":"2025-06-27T13:26:15.622291Z","iopub.status.idle":"2025-06-27T13:26:20.666193Z","shell.execute_reply.started":"2025-06-27T13:26:15.622255Z","shell.execute_reply":"2025-06-27T13:26:20.664602Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"---\n\n## ğŸ¤– BERTScore Evaluation Function\n\n```python\ndef evaluate_bertscore(reference, summary):\n    P, R, F1 = score([summary], [reference], lang='en', verbose=False)\n    return {\"precision\": P[0].item(), \"recall\": R[0].item(), \"f1\": F1[0].item()}\n```\n\n* `score(...)`: Computes semantic similarity between summary and reference.\n* Outputs: Precision, Recall, and F1 Score.","metadata":{}},{"cell_type":"code","source":"def evaluate_bertscore(reference, summary):\n    P, R, F1 = score([summary], [reference], lang='en', verbose=False)\n    return {\"precision\": P[0].item(), \"recall\": R[0].item(), \"f1\": F1[0].item()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:26:20.668216Z","iopub.execute_input":"2025-06-27T13:26:20.668957Z","iopub.status.idle":"2025-06-27T13:26:20.687156Z","shell.execute_reply.started":"2025-06-27T13:26:20.668860Z","shell.execute_reply":"2025-06-27T13:26:20.686154Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"---\n\n## ğŸ” Main Execution Block\n\n```python\nif __name__ == \"__main__\":\n```\n\n* Ensures the code only runs when the script is executed directly.\n\n### Original Text\n\n```python\noriginal_text = \"\"\" ... \"\"\"\n```\n\n* A sample passage describing the Apollo space program.\n\n### Generate Summary\n\n```python\nsummary = summarize(original_text)\n```\n\n* The input text is summarized using Pegasus.\n\n### Compute ROUGE Score\n\n```python\nrouge_scores = evaluate_rouge(original_text, summary)\n```\n\n### Compute BERTScore\n\n```python\nbert_scores = evaluate_bertscore(original_text, summary)\n```\n\n### Display Evaluation Scores\n\n```python\nprint(\"\\nğŸ“Š ROUGE Scores:\")\n...\nprint(\"\\nğŸ¤– BERTScore:\")\n...\n```\n\n* Prints out ROUGE and BERTScore metrics to evaluate summary quality.","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \n    original_text = \"\"\"\n    The Apollo program was the third United States human spaceflight program carried out by NASA,\n    which accomplished landing the first humans on the Moon from 1969 to 1972.\n    First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury,\n    which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s,\n    which he proposed in a May 25, 1961, address to Congress.\n    \"\"\"\n\n    print(\"ğŸ“„ Original Text:\\n\", original_text.strip())\n\n     \n    summary = summarize(original_text)\n    print(\"\\nâœ‚ï¸ Generated Summary:\\n\", summary)\n\n    \n    rouge_scores = evaluate_rouge(original_text, summary)\n    bert_scores = evaluate_bertscore(original_text, summary)\n\n    \n    print(\"\\nğŸ“Š ROUGE Scores:\")\n    for k, v in rouge_scores.items():\n        print(f\"{k.upper()}: Precision={v.precision:.4f}, Recall={v.recall:.4f}, F1={v.fmeasure:.4f}\")\n\n    \n    print(\"\\nğŸ¤– BERTScore:\")\n    print(f\"Precision: {bert_scores['precision']:.4f}\")\n    print(f\"Recall:    {bert_scores['recall']:.4f}\")\n    print(f\"F1 Score:  {bert_scores['f1']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:26:20.688333Z","iopub.execute_input":"2025-06-27T13:26:20.688713Z","iopub.status.idle":"2025-06-27T13:27:04.522884Z","shell.execute_reply.started":"2025-06-27T13:26:20.688681Z","shell.execute_reply":"2025-06-27T13:27:04.522013Z"}},"outputs":[{"name":"stdout","text":"ğŸ“„ Original Text:\n The Apollo program was the third United States human spaceflight program carried out by NASA,\n    which accomplished landing the first humans on the Moon from 1969 to 1972.\n    First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury,\n    which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s,\n    which he proposed in a May 25, 1961, address to Congress.\n\nâœ‚ï¸ Generated Summary:\n The Apollo program was the third U.S. human spaceflight program carried out by NASA . It landed the first humans on the Moon from 1969 to 1972 . It was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd64b1a0b6124ad585c660bced9095b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fe97dc9163e41248c260590a76a126a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba04e17e694549da87c495df94eeadf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a62799b1e947a6b445a10548a97f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"324d7554b1c44e8397f14ad9867e989a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475bb24bdb8749b0b458ad7fe99a9703"}},"metadata":{}},{"name":"stdout","text":"\nğŸ“Š ROUGE Scores:\nROUGE1: Precision=0.9348, Recall=0.4388, F1=0.5972\nROUGE2: Precision=0.8444, Recall=0.3918, F1=0.5352\nROUGEL: Precision=0.9130, Recall=0.4286, F1=0.5833\n\nğŸ¤– BERTScore:\nPrecision: 0.9537\nRecall:    0.8692\nF1 Score:  0.9095\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"---\n\n## Assessing the Quality of Summarization with ROUGE and BERTScore\n\n## ğŸ“„ **Original Text**\n\nThe original passage discusses the **Apollo program**, focusing on its historical context and purpose. It highlights NASA's achievement of landing humans on the Moon, as well as the national goals shaped during the Eisenhower and Kennedy administrations.\n\n---\n\n## âœ‚ï¸ **Generated Summary**\n\n> **\"The Apollo program was the third U.S. human spaceflight program carried out by NASA. It landed the first humans on the Moon from 1969 to 1972. It was later dedicated to President John F. Kennedy's national goal of 'landing a man on the Moon'\"**\n\n### ğŸ” Content Evaluation:\n\n* **Strengths**:\n\n  * Preserves the main ideas: the programâ€™s identity, timeline, NASAâ€™s involvement, and Kennedyâ€™s role are clearly conveyed.\n  * The summary is short, informative, and to the point.\n\n* **Weaknesses**:\n\n  * The final sentence seems incomplete or abruptly cut off.\n  * No mention of Eisenhower's initial role in conceptualizing the program.\n  * Lacks historical context, such as Kennedyâ€™s 1961 speech to Congress.\n\n---\n\n## ğŸ“Š **Interpretation of ROUGE Scores**\n\n| Metric  | Precision | Recall | F1 Score |\n| ------- | --------- | ------ | -------- |\n| ROUGE-1 | 0.9348    | 0.4388 | 0.5972   |\n| ROUGE-2 | 0.8444    | 0.3918 | 0.5352   |\n| ROUGE-L | 0.9130    | 0.4286 | 0.5833   |\n\n### ğŸ“Œ Explanation:\n\n* **Precision**: Measures how much of the generated summaryâ€™s content overlaps with the reference.\n\n  * High: 0.93 (ROUGE-1) â†’ Most of the words in the summary appear in the original text.\n* **Recall**: Measures how much of the important content from the reference is captured in the summary.\n\n  * Relatively low: \\~0.43 â†’ The summary misses several details from the original.\n* **F1 Score**: Harmonic mean of Precision and Recall.\n\n  * Moderate: \\~0.58 â†’ The summary is informative but omits key elements.\n\n---\n\n## ğŸ¤– **BERTScore Evaluation**\n\n| Metric    | Value  |\n| --------- | ------ |\n| Precision | 0.9537 |\n| Recall    | 0.8692 |\n| F1 Score  | 0.9095 |\n\n### ğŸ“Œ Explanation:\n\n* BERTScore assesses how semantically similar the generated summary is to the original text.\n* **Very high scores** â†’ The summary retains most of the intended meaning.\n* This indicates that Pegasus performs well in capturing the essence of the text, even if some details are missing.\n\n---\n\n## âœ… **Overall Evaluation**\n\n* The summary is **informative** and **semantically accurate**, but lacks a few important details and ends with an incomplete sentence.\n* **ROUGE scores** reveal that while the surface-level word overlap is solid, the coverage of content is somewhat limited.\n* **BERTScore results** confirm that the summary aligns well with the original meaning.\n","metadata":{}},{"cell_type":"markdown","source":"---\n\n## ğŸ¯ Purpose\n\nThis script has two primary goals:\n\n1. To summarize long-form text using the Pegasus transformer model.\n2. To evaluate the quality of the generated summary using both lexical (ROUGE) and semantic (BERTScore) metrics.\n\n---\n\n### âœ… **Conclusion**\n\nThis code offers a practical solution for summarizing long texts using a state-of-the-art transformer model and evaluating the results with both traditional and semantic-based metrics. ROUGE provides insight into surface-level overlap, while BERTScore measures deeper semantic similarity. Together, they offer a comprehensive assessment of summarization quality, making this approach highly suitable for academic, journalistic, or real-world NLP tasks.","metadata":{}}]}